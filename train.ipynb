{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e27f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc18ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: https://www.kaggle.com/datasets/rohitsuresh15/radroad-anomaly-detection\n",
    "\n",
    "DATASET_SOURCES = [\"./archive\"]\n",
    "\n",
    "FINAL_DATASET = Path(\"RAD_DATASET\")\n",
    "\n",
    "SPLITS = [\"train\", \"valid\", \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d88d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in SPLITS:\n",
    "    (FINAL_DATASET / \"images\" / split).mkdir(parents=True, exist_ok=True)\n",
    "    (FINAL_DATASET / \"labels\" / split).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2414e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_valid(img_path):\n",
    "    try:\n",
    "        with Image.open(img_path) as img:\n",
    "            img.verify()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_label_valid(label_path):\n",
    "    try:\n",
    "        with open(label_path) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        if len(lines) == 0:\n",
    "            return False\n",
    "\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 5:\n",
    "                return False\n",
    "\n",
    "            cls, x, y, w, h = parts\n",
    "            cls = int(cls)\n",
    "            x, y, w, h = map(float, [x, y, w, h])\n",
    "\n",
    "            if not (0 <= x <= 1 and 0 <= y <= 1 and 0 < w <= 1 and 0 < h <= 1):\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f48c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "discarded = 0\n",
    "kept = 0\n",
    "counter = 0\n",
    "\n",
    "for src in DATASET_SOURCES:\n",
    "    src = Path(src)\n",
    "\n",
    "    for split in SPLITS:\n",
    "        img_dir = src / split / \"images\"\n",
    "        lbl_dir = src / split / \"labels\"\n",
    "\n",
    "        if not img_dir.exists() or not lbl_dir.exists():\n",
    "            continue\n",
    "\n",
    "        for img_file in tqdm(list(img_dir.glob(\"*\"))):\n",
    "            label_file = lbl_dir / (img_file.stem + \".txt\")\n",
    "\n",
    "            # Missing label\n",
    "            if not label_file.exists():\n",
    "                discarded += 1\n",
    "                continue\n",
    "\n",
    "            # Corrupt image\n",
    "            if not is_image_valid(img_file):\n",
    "                discarded += 1\n",
    "                continue\n",
    "\n",
    "            # Invalid label\n",
    "            if not is_label_valid(label_file):\n",
    "                discarded += 1\n",
    "                continue\n",
    "\n",
    "            # Passed all checks copy\n",
    "            new_img_name = f\"{img_file.stem}_{counter}{img_file.suffix}\"\n",
    "            new_lbl_name = f\"{img_file.stem}_{counter}.txt\"\n",
    "\n",
    "            shutil.copy(img_file, FINAL_DATASET / \"images\" / split / new_img_name)\n",
    "            shutil.copy(label_file, FINAL_DATASET / \"labels\" / split / new_lbl_name)\n",
    "\n",
    "            counter += 1\n",
    "            kept += 1\n",
    "\n",
    "print(f\"Kept samples: {kept}\")\n",
    "print(f\"Discarded samples: {discarded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d9b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data.yaml\n",
    "DATA_YAML = FINAL_DATASET / \"data.yaml\"\n",
    "\n",
    "# Load YAML\n",
    "with open(DATA_YAML) as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "# Class names\n",
    "class_names = data[\"names\"]  # list\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Initialize counters\n",
    "class_counts = {name: 0 for name in class_names}\n",
    "\n",
    "# Loop through all label files (train + valid + test)\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    lbl_dir = FINAL_DATASET / \"labels\" / split\n",
    "    if not lbl_dir.exists():\n",
    "        continue\n",
    "\n",
    "    for lbl_file in lbl_dir.glob(\"*.txt\"):\n",
    "        with open(lbl_file) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if line.strip() == \"\":\n",
    "                    continue\n",
    "                class_id = int(line.split()[0])\n",
    "                class_name = class_names[class_id]\n",
    "                class_counts[class_name] += 1\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(class_counts.keys(), class_counts.values(), color=\"skyblue\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Number of instances\")\n",
    "plt.title(\"Class Distribution in Final Dataset\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Print class counts\n",
    "print(\"Class distribution:\")\n",
    "for cls, count in class_counts.items():\n",
    "    print(f\"{cls}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e17d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "TRAIN_IMAGES = FINAL_DATASET / \"images\" / \"train\"\n",
    "TRAIN_LABELS = FINAL_DATASET / \"labels\" / \"train\"\n",
    "DATA_YAML = FINAL_DATASET / \"data.yaml\"\n",
    "\n",
    "# Load class names\n",
    "import yaml\n",
    "with open(DATA_YAML) as f:\n",
    "    data = yaml.safe_load(f)\n",
    "class_names = data[\"names\"]\n",
    "\n",
    "# Count current train images per class\n",
    "class_counts = {name: 0 for name in class_names}\n",
    "for lbl_file in TRAIN_LABELS.glob(\"*.txt\"):\n",
    "    with open(lbl_file) as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "            class_id = int(line.split()[0])\n",
    "            class_counts[class_names[class_id]] += 1\n",
    "\n",
    "print(\"Train class counts before augmentation:\", class_counts)\n",
    "\n",
    "# Target number of images for minority classes\n",
    "target_count = 5000\n",
    "\n",
    "# Albumentations augmentation pipeline\n",
    "aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.Rotate(limit=20, p=0.5),\n",
    "    A.RandomScale(scale_limit=0.2, p=0.5),\n",
    "])\n",
    "\n",
    "# Augment minority classes\n",
    "for cls_name, count in class_counts.items():\n",
    "    if count >= target_count:\n",
    "        continue  # already enough images\n",
    "\n",
    "    n_needed = target_count - count\n",
    "    print(f\"Augmenting {cls_name}: +{n_needed} images\")\n",
    "\n",
    "    # Find all images containing this class\n",
    "    images_with_class = []\n",
    "    for lbl_file in TRAIN_LABELS.glob(\"*.txt\"):\n",
    "        with open(lbl_file) as f:\n",
    "            for line in f:\n",
    "                if line.strip() == \"\":\n",
    "                    continue\n",
    "                if class_names[int(line.split()[0])] == cls_name:\n",
    "                    images_with_class.append(lbl_file.stem)\n",
    "                    break\n",
    "\n",
    "    # Perform augmentation\n",
    "    for i in range(n_needed):\n",
    "        img_stem = random.choice(images_with_class)\n",
    "        img_path = TRAIN_IMAGES / f\"{img_stem}.jpg\"\n",
    "        lbl_path = TRAIN_LABELS / f\"{img_stem}.txt\"\n",
    "\n",
    "        # Read image\n",
    "        img = cv2.imread(str(img_path))\n",
    "        augmented = aug(image=img)\n",
    "        aug_img = augmented[\"image\"]\n",
    "\n",
    "        # Save augmented image\n",
    "        new_img_name = f\"{img_stem}_aug{i}.jpg\"\n",
    "        cv2.imwrite(str(TRAIN_IMAGES / new_img_name), aug_img)\n",
    "\n",
    "        # Copy label file\n",
    "        shutil.copy(lbl_path, TRAIN_LABELS / f\"{img_stem}_aug{i}.txt\")\n",
    "\n",
    "# Recount after augmentation\n",
    "class_counts_after = {name: 0 for name in class_names}\n",
    "for lbl_file in TRAIN_LABELS.glob(\"*.txt\"):\n",
    "    with open(lbl_file) as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "            class_id = int(line.split()[0])\n",
    "            class_counts_after[class_names[class_id]] += 1\n",
    "\n",
    "print(\"Train class counts after augmentation:\", class_counts_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12df9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to dataset and YAML\n",
    "DATA_YAML = FINAL_DATASET / \"data.yaml\"\n",
    " \n",
    "# Initialize YOLO11s model\n",
    "model = YOLO('yolo11s.pt')  # pretrained YOLOv11s model\n",
    "\n",
    "# Train the model\n",
    "model.train(\n",
    "    data=str(DATA_YAML),\n",
    "    epochs=100,\n",
    "    imgsz=320,\n",
    "    batch=16,\n",
    "    name=\"yolov11s_trained\",\n",
    "    device=0,           # GPU ID, set -1 for CPU\n",
    "    workers=4,\n",
    "    optimizer='SGD',    # or 'Adam', default is SGD\n",
    "    augment=True,       # enables Mosaic, MixUp, flips, etc.\n",
    "    plots=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7398ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('./runs/detect/yolov11s_trained/weights/best.pt') \n",
    "\n",
    "model.export(format='tflite')\n",
    "model.export(format='tflite', int8=True, data=str(DATA_YAML))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\nEvaluating model on test images...\")\n",
    "\n",
    "best_model_path = './runs/detect/yolov11s_trained/weights/best.pt'\n",
    "\n",
    "# Load trained model\n",
    "best_model = YOLO(str(best_model_path))\n",
    "\n",
    "try:\n",
    "    # Run evaluation on test split\n",
    "    metrics = best_model.val(data=str(DATA_YAML), split=\"test\", device=\"cpu\")\n",
    "\n",
    "    # Get class names safely\n",
    "    class_names_from_model = getattr(best_model, \"names\", {})\n",
    "\n",
    "    # Display results\n",
    "    display_results(metrics, class_names_from_model)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "\n",
    "print(\"\\nModel training and evaluation complete!\")\n",
    "print(f\"Your best trained model is saved at: {best_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
